{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN_for_Network_Resiliance.ipynb","version":"0.3.2","provenance":[{"file_id":"1wse7x5SrK1BRsEZhytshZcDcVqp8myU3","timestamp":1562154776730}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ywLDndLFGsni","colab_type":"text"},"source":["\n","#Artificial Neural Networks for classification of the Resilience in WRONs (Wavelength Routed Optical Networks) through Double and Simple Failures\n","\n","\n","---\n","\n","\n","<strong>Authors:</strong>\n","\n","  * Christian Lira (christian.lira@ufrpe.br)\n","  * Jonas Freire (jonas.freire@gmail.com)\n","  * Pedro Araújo (pedro.araujonascimento@ufrpe.br)\n"]},{"cell_type":"markdown","metadata":{"id":"EQZwBtpbdWfy","colab_type":"text"},"source":["###Necessaries Packages"]},{"cell_type":"code","metadata":{"id":"WlFtsAqPuHmk","colab_type":"code","outputId":"8908dac6-f265-40dd-89af-8a1438697267","executionInfo":{"status":"ok","timestamp":1563325359208,"user_tz":180,"elapsed":3283,"user":{"displayName":"Rose","photoUrl":"https://lh6.googleusercontent.com/-1CMMwFg8u3A/AAAAAAAAAAI/AAAAAAAAAa8/FGzptYDhSHM/s64/photo.jpg","userId":"12693359545686293983"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import pandas as pd\n","\n","import re\n","import time\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.utils import to_categorical\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import accuracy_score\n","\n","import io\n","from contextlib import redirect_stdout"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"s3grH3DRhtVb","colab_type":"text"},"source":["##Functions"]},{"cell_type":"markdown","metadata":{"id":"3mB5UIXBkvYx","colab_type":"text"},"source":["###Preprocessing Data Function"]},{"cell_type":"code","metadata":{"id":"PHqYBKtLk6Xm","colab_type":"code","colab":{}},"source":["\"\"\"\n","Função preprocessing: Faz o pré-processamento da base de dados.\n","- Limpa os dados\n","- Separa as métricas\n","- Gera arquivo de nome \"dataset.csv\"\n","\n","Entrada: Lista com o path dos arquivos\n","\"\"\"\n","def preprocessing(filenames):\n","  \n","  # limpando os dados e juntando no arquivo 'dataset.txt'\n","  data = \"\"\n","  for file in filenames:\n","    arq = open(file, 'r')\n","    data += arq.read()\n","    arq.close()\n","  \n","  regex = r\"(;\\n|;$)\"\n","  clean_data = re.sub(regex, '\\n', data)\n","  \n","  arq = open('data.txt', 'w')\n","  arq.write(clean_data)\n","  arq.close()\n","  \n","  # separando e normalizando as métricas\n","  dataframe = pd.read_csv('data.txt', header=None, sep=';')\n","  database = dataframe.values\n","  \n","  # taking the metric \"Algebraic Connectivity\"\n","  algebraic_connectivity = database[:, 4:5]\n","  \n","  # taking the metric \"Natural Connectivity\"\n","  natural_connectivity = database[:, 5:6]\n","  \n","  # taking the metric \"DFT of Laplacian Entropy\"\n","  dft_laplacian_entropy = database[:, 14:15]\n","  \n","  # taking the metric \"Number of Nodes\"\n","  list = [[18]] * len(algebraic_connectivity)\n","  num_nodes = np.array(list)\n","  \n","  # taking the metric \"Number of Links\"\n","  list = []\n","    \n","  for mtx_adj in database[:, 15:16]:\n","    count = 0\n","    for vet_adj in mtx_adj:\n","      for cel in vet_adj:\n","        if cel == '1':\n","          count += 1\n","    \n","    list.append([count])\n","  \n","  num_links = np.array(list)\n","  \n","  # taking the metric \"Hub Degree\"\n","  mtx_adj = []\n","  list = []\n","  i_complete = 0\n","  j_complete = 0\n","  count = 0\n","  links_count= 0\n","  max_connections = 0\n","  \n","  # building the superior matrix\n","  for np_adj in database[:, 15:16]:\n","    for str_adj in np_adj:\n","      vet_adj = str_adj.split(' ')\n","      count = 0\n","      links_count = 0\n","      max_connections = 0\n","      mtx_adj = []\n","      for i in range(18):\n","        i_complete = i\n","        j_complete = 17 - i\n","        lista = []\n","        for j in range(18):\n","          if i == j:\n","            lista.append(0)\n","            i_complete -= 1\n","          else:\n","            if i_complete >= 0:\n","              lista.append(-1)\n","              i_complete -= 1\n","            elif j_complete >= 0:\n","              lista.append(int(vet_adj[count]))\n","              count += 1\n","              j_complete -= 1\n","              \n","        mtx_adj.append(lista)\n","        \n","      # building the full matrix\n","      for i in range(18):\n","        for j in range(18):\n","          if mtx_adj[i][j] == -1:\n","            mtx_adj[i][j] = mtx_adj[j][i]\n","      \n","      for i in range(18):\n","        for j in range(18):\n","          if mtx_adj[i][j] == 1:\n","            links_count += 1           \n","        if links_count > max_connections:\n","          max_connections = links_count\n","        links_count = 0\n","      \n","      list.append([max_connections])\n","  \n","  hub_degree = np.array(list)\n","  \n","  # taking the class \"Robustness for simple failures\"\n","  simple_failures_robustness = database[:, 30:31]\n","  \n","  # taking the class \"Robustness for double failures\"\n","  double_failures_robustness = database[:, 31:32]\n","  \n","  dataset = []\n","  list = []  \n","  \n","  for i in range(len(database)):\n","    list.append(algebraic_connectivity[i].tolist()[0])\n","    list.append(natural_connectivity[i].tolist()[0])\n","    list.append(dft_laplacian_entropy[i].tolist()[0])\n","    list.append(num_nodes[i].tolist()[0])\n","    list.append(num_links[i].tolist()[0])\n","    list.append(hub_degree[i].tolist()[0])\n","    list.append(simple_failures_robustness[i].tolist()[0]) \n","    list.append(double_failures_robustness[i].tolist()[0])\n","    dataset.append(list)\n","    list = []\n","    \n","  my_df = pd.DataFrame(dataset)\n","  my_df.to_csv('dataset.csv', index=False, header=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szEHZxmc8P-m","colab_type":"text"},"source":["###Load Data Function"]},{"cell_type":"code","metadata":{"id":"Aph80tjV8PTf","colab_type":"code","colab":{}},"source":["\"\"\"\n","\n","Função which_class: Diz qual à classe um exemplo pertence.\n","Entrada: Exemplo do dataset e coluna com a classe\n","Saída: Classe\n","\n","\"\"\"\n","def whichclass(exemple, col):\n","  \n","  if exemple[col] >= 0.0 and exemple[col] < 0.1:\n","    return(0)\n","  if exemple[col] >= 0.1 and exemple[col] < 0.2:\n","    return(1)\n","  if exemple[col] >= 0.2 and exemple[col] < 0.3:\n","    return(2)\n","  if exemple[col] >= 0.3 and exemple[col] < 0.4:\n","    return(3)\n","  if exemple[col] >= 0.4 and exemple[col] < 0.5:\n","    return(4)\n","  if exemple[col] >= 0.5 and exemple[col] < 0.6:\n","    return(5)\n","  if exemple[col] >= 0.6 and exemple[col] < 0.7:\n","    return(6)\n","  if exemple[col] >= 0.7 and exemple[col] < 0.8:\n","    return(7)\n","  if exemple[col] >= 0.8 and exemple[col] < 0.9:\n","    return(8)\n","  if exemple[col] >= 0.9 and exemple[col] <= 1.0:\n","    return(9)\n","\n","  \n","\"\"\"\n","\n","Função loadData: Normaliza o dataset e faz a separabilidade dos conjuntos.\n","\n","Entrada: Path do arquivo \"dataset.csv\"\n","Saídas: Tupla com os valores\n","  - x_simple_train: Vetor de características do conjunto de treino de falhas simples\n","  - y_simple_train: Classe do conjunto de treino de falhas simples\n","  - x_simple_test: Vetor de características do conjunto de testes de falhas simples\n","  - y_simple_test: Classe do conjunto de testes de falhas simples\n","  - x_double_train: Vetor de características do conjunto de treino de falhas duplas\n","  - y_double_train: Classe do conjunto de treino de falhas duplas\n","  - x_double_test: Vetor de características do conjunto de testes de falhas duplas\n","  - y_double_test: Classe do conjunto de testes de falhas duplas\n","  \n","\"\"\"\n","def loadData(path):\n","  \n","  dataframe = pd.read_csv(path, header=None, sep=',')\n","  database = dataframe.values\n","    \n","  # Normalização\n","  for col in range(len(database[0])):\n","    if type(database[0][col]) == str:\n","      max = float(database[0][col].replace('.', '').replace(',', '.'))\n","      min = float(database[0][col].replace('.', '').replace(',', '.'))\n","      for lin in range(len(database)):\n","        line_value = float(database[lin][col].replace('.', '').replace(',', '.'))\n","        if line_value > max:\n","          max = line_value\n","        if line_value < min:\n","          min = line_value\n","          \n","      variancia = max - min\n","      \n","      for lin in range(len(database)):\n","        line_value = float(database[lin][col].replace('.', '').replace(',', '.'))\n","        database[lin][col] = float((line_value - min) / variancia)\n","        \n","    elif type(database[0][col]) == int:\n","      max = float(database[0][col])\n","      min = float(database[0][col])\n","      for lin in range(len(database)):\n","        line_value = float(database[lin][col])\n","        if line_value > max:\n","          max = line_value\n","        if line_value < min:\n","          min = line_value\n","          \n","      variancia = max - min\n","      \n","      if variancia == 0:\n","        for lin in range(len(database)):\n","          if max == 0:\n","            database[lin][col] = 0.0\n","          else:\n","            database[lin][col] = 1.0\n","      else:              \n","        for lin in range(len(database)):\n","          line_value = float(database[lin][col])\n","          database[lin][col] = float((line_value - min) / variancia)\n","    \n","    elif type(database[0][col]) == float:\n","      max = database[0][col]\n","      min = database[0][col]\n","      for lin in range(len(database)):\n","        line_value = database[lin][col]\n","        if line_value > max:\n","          max = line_value\n","        if line_value < min:\n","          min = line_value\n","          \n","      variancia = max - min\n","      \n","      for lin in range(len(database)):\n","        line_value = database[lin][col]\n","        database[lin][col] = float((line_value - min) / variancia)\n","  \n","  # Separabilidade das classes\n","  qtd_exemples = len(database)\n","  qtd_classes = np.zeros(10, dtype=int)\n","  \n","  for i in range(qtd_exemples):\n","    if database[i][6] >= 0.0 and database[i][6] < 0.1:\n","      qtd_classes[0] += 1\n","    elif database[i][6] >= 0.1 and database[i][6] < 0.2:\n","      qtd_classes[1] += 1\n","    elif database[i][6] >= 0.2 and database[i][6] < 0.3:\n","      qtd_classes[2] += 1\n","    elif database[i][6] >= 0.3 and database[i][6] < 0.4:\n","      qtd_classes[3] += 1\n","    elif database[i][6] >= 0.4 and database[i][6] < 0.5:\n","      qtd_classes[4] += 1\n","    elif database[i][6] >= 0.5 and database[i][6] < 0.6:\n","      qtd_classes[5] += 1\n","    elif database[i][6] >= 0.6 and database[i][6] < 0.7:\n","      qtd_classes[6] += 1\n","    elif database[i][6] >= 0.7 and database[i][6] < 0.8:\n","      qtd_classes[7] += 1\n","    elif database[i][6] >= 0.8 and database[i][6] < 0.9:\n","      qtd_classes[8] += 1\n","    elif database[i][6] >= 0.9 and database[i][6] <= 1.0:\n","      qtd_classes[9] += 1\n","  \n","  qtd_classes_conj_treino = np.zeros(10, dtype=int)\n","  qtd_classes_conj_teste = np.zeros(10, dtype=int)\n","\n","  for i in range(len(qtd_classes)):\n","    if int(qtd_classes[i] * 0.75) == 0 and qtd_classes[i] != 0:\n","      qtd_classes_conj_treino[i] = 1\n","    else:\n","      qtd_classes_conj_treino[i] = qtd_classes[i] * 0.75\n","    qtd_classes_conj_teste[i] = qtd_classes[i] - qtd_classes_conj_treino[i]  \n","  \n","  train = []\n","  teste = []\n","  \n","  qtd_classe_treino_add = np.zeros(10,dtype=int)\n","  qtd_classe_teste_add = np.zeros(10, dtype=int)\n","\n","  np.random.shuffle(database)\n","  for i in range(qtd_exemples):\n","    classe = newwhichclass(database[i], 6)\n","    \n","    if qtd_classes_conj_treino[classe] > 0:\n","      train.append(database[i].tolist())\n","      qtd_classes_conj_treino[classe] -= 1\n","    else:\n","      teste.append(database[i].tolist())\n","      qtd_classes_conj_teste[classe] -= 1\n","      \n","  simple_train = np.asarray(train)\n","  simple_teste = np.asarray(teste)\n","    \n","  x_train = simple_train[:, 0:6]\n","  y_train = simple_train[:, 6:]\n","  x_test = simple_teste[:, 0:6]\n","  y_test = simple_teste[:, 6:]\n","    \n","  return(x_train, y_train, x_test, y_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K85ntVNmKO-G","colab_type":"text"},"source":["##ANN"]},{"cell_type":"markdown","metadata":{"id":"pkwZ4cWceast","colab_type":"text"},"source":["### Loop for test different quantities of neuros in hidden layer"]},{"cell_type":"code","metadata":{"id":"L0AZ8EpWe1kR","colab_type":"code","colab":{}},"source":["filenames = ['hessen_shuffle 0.txt', 'hessen_shuffle 1.txt', 'hessen_shuffle_2.txt']\n","\n","average_accuracy = 0\n","average_mse = 0\n","\n","sum_acc = 0\n","sum_mse = 0\n","\n","list_acc = []\n","list_mse = []\n","\n","for i in range(5, 51, 5):\n","\n","  preprocessing(filenames)\n","  x_train, y_train, x_test, y_test = newloadData('dataset.csv')\n","\n","  model = Sequential()\n","  model.add(Dense(units = i, activation = 'relu', input_dim = 6))\n","  model.add(Dense(units = 2, activation = 'sigmoid'))\n","  model.compile(optimizer = 'adam', loss = 'mse', metrics = ['acc'])\n","    \n","  model.fit(x_train, y_train, batch_size = 10, epochs = 500, verbose = 0)\n","\n","  score = model.evaluate(x_test, y_test)\n","  list_mse.append(score[0])\n","  list_acc.append(score[1])\n","\n","for i in range(len(list_acc)):\n","  sum_acc += list_acc[i]\n","  sum_mse += list_mse[i]\n","  \n","average_accuracy = sum_acc / len(list_acc)\n","average_mse = sum_mse / len(list_mse)\n","\n","print('Média de Acurácia ')\n","print(average_accuracy)\n","print('Média de MSE ')\n","print(average_mse)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6huyiTkj4Km","colab_type":"text"},"source":["##Plotting "]},{"cell_type":"code","metadata":{"id":"FcR7jXdV1La9","colab_type":"code","colab":{}},"source":["\n","filenames = ['hessen_shuffle 0.txt', 'hessen_shuffle 1.txt', 'hessen_shuffle_2.txt']\n","\n","preprocessing(filenames)\n","x_train, y_train, x_test, y_test = newloadData('dataset.csv')\n","\n","model = Sequential()\n","model.add(Dense(units = 45, activation = 'relu', input_dim = 6))\n","model.add(Dense(units = 2, activation = 'sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'mse', metrics = ['acc'])\n","    \n","history = model.fit(x_train, y_train, batch_size = 10, epochs = 500, verbose = 0)\n","score = model.evaluate(x_test, y_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFiIDZY7UaLk","colab_type":"text"},"source":["###SpeedUp ANN vs SIMTON"]},{"cell_type":"code","metadata":{"id":"s0E-ltOEj9YR","colab_type":"code","colab":{}},"source":["predict_time_us = []\n","predict_time_ms = []\n","simtom_time = 44382.98886167756\n","sum_predict = 0\n","average = 0\n","\n","for i in range(100):\n","  f = io.StringIO()\n","  with redirect_stdout(f):\n","    previsions = model.predict(x_test[0:1, :], verbose = 1)\n","  \n","  s = f.getvalue()\n","  if 'm' in s.split()[4]:\n","    predict_time_ms.append(float(s.split()[4].split('m')[0]))\n","  else:\n","    predict_time_us.append(float(s.split()[4].split('u')[0]))\n","  \n","\n","for element in map(lambda x : x * 1000, predict_time_ms):\n","  predict_time_us.append(element)\n","  \n","for i in predict_time_us:\n","  sum_predict += i\n","\n","average = sum_predict / len(predict_time_us)\n","print(predict_time_us)\n","print('Tempo de Predição ANN')\n","print(average, 'us')\n","print(average / 1000)\n","print('Tempo de Predição SIMTOM')\n","print(simtom_time, 'ms')\n","print('SpeedUp')\n","print(simtom_time / (average / 1000))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VGlMVtTFUi3U","colab_type":"text"},"source":["###Generate Graphics"]},{"cell_type":"code","metadata":{"id":"_yUw09g7UjqU","colab_type":"code","colab":{}},"source":["previsions = model.predict(x_test, verbose = 1)\n","\n","print(previsions)\n","print(score[1])\n","print(score[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qq4S3Q0ZUA8H","colab_type":"text"},"source":["###Accuracy Over Epochs"]},{"cell_type":"code","metadata":{"id":"0HhKpisBUFJS","colab_type":"code","colab":{}},"source":["plt.plot(history.history['acc'])\n","plt.title('Acurácia Durante Épocas')\n","plt.ylabel('acurácia')\n","plt.xlabel('épocas')\n","plt.legend(['treino'], loc='lower right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cY7bdo0UI57","colab_type":"text"},"source":["###Loss Over Epochs"]},{"cell_type":"code","metadata":{"id":"WiYa8SEuULNU","colab_type":"code","colab":{}},"source":["plt.plot(history.history['loss'])\n","plt.title('Erro Durante Épocas')\n","plt.ylabel('erro')\n","plt.xlabel('épocas')\n","plt.legend(['treino'], loc='best')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXQao9LFLL6E","colab_type":"text"},"source":["###Accuracy vs Nº of Neurons (Simple Failures)"]},{"cell_type":"code","metadata":{"id":"-lS5qnIuArWy","colab_type":"code","colab":{}},"source":["acc = [0.9747447073489310, 0.9682025737129538, 0.9701535907349386,\n","      0.9717310087228770, 0.9725963154103080,0.9733913333,\n","      0.9720215, 0.9710460772197390, 0.96326267, 0.9710253217]\n","\n","num = [5,10,15,20,25,30,35,40,45,50]\n","\n","plt.title('Accuracy x Number of Neurons')\n","plt.xlabel('nº of neurons')\n","plt.ylabel('accuracy')\n","plt.plot(num,acc)\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMuzuL_AOw8O","colab_type":"text"},"source":["###MSE vs Nº of Neurons (Simple Failures)"]},{"cell_type":"code","metadata":{"id":"gWOgLJ7OO1cY","colab_type":"code","colab":{}},"source":["mse = [0.0032382364847859000, 0.0030215065701311434, 0.002998395328865773,\n","      0.0030937843121529000, 0.0030426691457763300,0.002993633333,\n","      0.002995133333, 0.0029131011183504300, 0.00298817, 0.002994107197]\n","\n","num = [5,10,15,20,25,30,35,40,45,50]\n","\n","plt.title('MSE x Number of Neurons')\n","plt.xlabel('nº of neurons')\n","plt.ylabel('mse')\n","plt.plot(num,mse)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdLf-vpJPPhT","colab_type":"text"},"source":["###ANN vs SIMTON (Simple Failures)"]},{"cell_type":"code","metadata":{"id":"ZWlDYYfjPSNj","colab_type":"code","colab":{}},"source":["plt.plot(previsions[150:203, 0:1])\n","plt.plot(y_test[150:203, 0:1])\n","plt.title('RNA vs SIMTON (Falhas Simples)')\n","plt.ylabel('Resiliência')\n","plt.legend(['RNA', 'SIMTON'], loc='best')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BumMBUFuUPBU","colab_type":"text"},"source":["###ANN vs SIMTON (Double Failures)"]},{"cell_type":"code","metadata":{"id":"iQmliUY0UTTF","colab_type":"code","colab":{}},"source":["plt.plot(previsions[600:700, 1:])\n","plt.plot(y_test[600:700, 1:])\n","plt.title('ANN vs SIMTOM (Double Failures)')\n","plt.ylabel('accuracy')\n","plt.legend(['ANN', 'SIMTOM'], loc='best')\n","plt.show()"],"execution_count":0,"outputs":[]}]}